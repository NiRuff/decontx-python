{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2a61c13a17ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what we can import\n",
    "import sys\n",
    "import decontx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def create_test_data(n_genes=500, n_cells=200, n_empty=1000, contamination=0.1, seed=42):\n",
    "    \"\"\"Create simple synthetic data for testing.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create true expression for 5 cell types\n",
    "    n_types = 5\n",
    "    cells_per_type = n_cells // n_types\n",
    "    true_counts = np.zeros((n_genes, n_cells))\n",
    "\n",
    "    # Each cell type expresses different genes\n",
    "    for cell_type in range(n_types):\n",
    "        start_cell = cell_type * cells_per_type\n",
    "        end_cell = min(start_cell + cells_per_type, n_cells)\n",
    "\n",
    "        # High expression for marker genes\n",
    "        marker_genes = range(cell_type * 20, (cell_type + 1) * 20)\n",
    "        for cell in range(start_cell, end_cell):\n",
    "            true_counts[list(marker_genes), cell] = np.random.poisson(20, 20)\n",
    "            # Low background\n",
    "            other_genes = list(set(range(n_genes)) - set(marker_genes))\n",
    "            true_counts[other_genes, cell] = np.random.poisson(0.5, len(other_genes))\n",
    "\n",
    "    # Create soup profile (average of all cells)\n",
    "    soup_profile = np.mean(true_counts, axis=1)\n",
    "    soup_profile = soup_profile / soup_profile.sum()\n",
    "\n",
    "    # Add contamination\n",
    "    observed = true_counts.copy()\n",
    "    for cell in range(n_cells):\n",
    "        cell_total = true_counts[:, cell].sum()\n",
    "        contam_counts = np.random.poisson(soup_profile * cell_total * contamination)\n",
    "        observed[:, cell] += contam_counts\n",
    "\n",
    "    # Create empty droplets\n",
    "    empty_counts = np.zeros((n_genes, n_empty))\n",
    "    for i in range(n_empty):\n",
    "        n_umis = np.random.poisson(10)  # Low UMI counts\n",
    "        if n_umis > 0:\n",
    "            empty_counts[:, i] = np.random.multinomial(n_umis, soup_profile)\n",
    "\n",
    "    # Combine for raw matrix\n",
    "    raw = np.hstack([empty_counts, observed])\n",
    "\n",
    "    return {\n",
    "        'raw': sp.csr_matrix(raw),\n",
    "        'filtered': sp.csr_matrix(observed),\n",
    "        'true': sp.csr_matrix(true_counts),\n",
    "        'genes': [f\"Gene_{i:04d}\" for i in range(n_genes)],\n",
    "        'cells': [f\"Cell_{i:04d}\" for i in range(n_cells)],\n",
    "        'contamination': contamination,\n",
    "        'cell_types': np.repeat(range(n_types), cells_per_type)[:n_cells]\n",
    "    }\n",
    "\n",
    "# Create test data\n",
    "data = create_test_data(n_genes=300, n_cells=100, contamination=0.15, seed=42)\n",
    "print(f\"Created data: {data['raw'].shape[0]} genes, {data['raw'].shape[1]} droplets\")\n",
    "print(f\"Filtered: {data['filtered'].shape[1]} cells\")\n",
    "print(f\"True contamination: {data['contamination']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d5b4f-7578-4b26-a000-b9d4b53ef808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean imports now\n",
    "from decontx import decontx, simulate_contamination\n",
    "import anndata as adata_module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"âœ“ Successfully imported decontx functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614ac42-fdb2-46f0-a86d-c4b44168433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData object from your test data\n",
    "adata = adata_module.AnnData(\n",
    "    X=data['filtered'].T,  # AnnData expects cells x genes\n",
    "    var=pd.DataFrame(index=data['genes']),\n",
    "    obs=pd.DataFrame(index=data['cells'])\n",
    ")\n",
    "\n",
    "# Add cell type info\n",
    "adata.obs['cell_type'] = data['cell_types']\n",
    "\n",
    "print(f\"Created AnnData: {adata.shape} (cells x genes)\")\n",
    "print(f\"Cell types: {adata.obs['cell_type'].unique()}\")\n",
    "print(f\"Data type: {type(adata.X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbbce6-3cb9-42b9-bf93-7635625fe6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic decontx test - expect this to break early!\n",
    "try:\n",
    "    print(\"Running basic decontx...\")\n",
    "    \n",
    "    result = decontx(\n",
    "        adata, \n",
    "        copy=True,\n",
    "        verbose=True,\n",
    "        max_iter=5,  # Very short for testing\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸŽ‰ It worked! (surprising)\")\n",
    "    print(f\"Result shape: {result.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ðŸ’¥ Expected error: {e}\")\n",
    "    \n",
    "    # Show just the error type and message, not full traceback yet\n",
    "    import traceback\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(\"Last few lines of traceback:\")\n",
    "    tb_lines = traceback.format_exc().split('\\n')\n",
    "    for line in tb_lines[-6:]:\n",
    "        if line.strip():\n",
    "            print(f\"  {line}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c47d9-d7f2-4c71-ba8c-eaf2927809ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the returned result object, not the original adata\n",
    "print(\"Results in returned object:\")\n",
    "print(f\"Layers: {list(result.layers.keys())}\")\n",
    "print(f\"Obs columns: {list(result.obs.columns)}\")\n",
    "\n",
    "# Check for contamination estimates\n",
    "if 'decontX_contamination' in result.obs:\n",
    "    print(f\"Contamination range: {result.obs['decontX_contamination'].min():.1%} - {result.obs['decontX_contamination'].max():.1%}\")\n",
    "    print(f\"Mean contamination: {result.obs['decontX_contamination'].mean():.1%}\")\n",
    "\n",
    "# Check for decontaminated counts\n",
    "if 'decontX_counts' in result.layers:\n",
    "    print(f\"Decontaminated matrix shape: {result.layers['decontX_counts'].shape}\")\n",
    "    print(f\"Original sum: {adata.X.sum():.0f}\")\n",
    "    print(f\"Decontaminated sum: {result.layers['decontX_counts'].sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd9fd7-ae04-4f98-b1e7-82ff1324e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare original vs decontaminated\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check a few marker genes\n",
    "marker_genes = [f\"Gene_{i:04d}\" for i in [0, 20, 40, 60, 80]]  # One from each cell type\n",
    "gene_idx = [result.var_names.get_loc(g) for g in marker_genes if g in result.var_names]\n",
    "\n",
    "if gene_idx:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Original expression\n",
    "    axes[0].boxplot([result.X[:, i].toarray().flatten() for i in gene_idx])\n",
    "    axes[0].set_title(\"Original Expression\")\n",
    "    axes[0].set_xticklabels([f\"Gene_{i*20}\" for i in range(len(gene_idx))])\n",
    "    \n",
    "    # Decontaminated expression  \n",
    "    axes[1].boxplot([result.layers['decontX_counts'][:, i] for i in gene_idx])\n",
    "    axes[1].set_title(\"Decontaminated Expression\")\n",
    "    axes[1].set_xticklabels([f\"Gene_{i*20}\" for i in range(len(gene_idx))])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3751ce-2a10-41c3-ab80-05a4dc8b2ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a39d4ab1-b328-4c77-92cf-4bbf6306a9ba",
   "metadata": {},
   "source": [
    "### PBMC3k testing - prepare with scanpy, then python vs R implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d7204-5c54-44eb-ae1c-fd455836c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tarfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.io import mmread\n",
    "\n",
    "def load_10x_data_fixed(raw_tar_path, filtered_tar_path, extract_dir=\"./temp_10x/\"):\n",
    "    \"\"\"\n",
    "    Fixed loader for 10X Genomics data from tar.gz files.\n",
    "    Properly handles matrix orientation - 10X matrices are stored as features x barcodes.\n",
    "    \"\"\"\n",
    "    # Create extraction directory\n",
    "    Path(extract_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    def find_matrix_files(base_dir):\n",
    "        \"\"\"Recursively find matrix.mtx, features.tsv/genes.tsv, barcodes.tsv\"\"\"\n",
    "        matrix_file = None\n",
    "        features_file = None\n",
    "        barcodes_file = None\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file == \"matrix.mtx\":\n",
    "                    matrix_file = file_path\n",
    "                elif file in [\"features.tsv\", \"genes.tsv\"]:\n",
    "                    features_file = file_path\n",
    "                elif file == \"barcodes.tsv\":\n",
    "                    barcodes_file = file_path\n",
    "        return matrix_file, features_file, barcodes_file\n",
    "\n",
    "    def extract_and_load_matrix(tar_path, matrix_name):\n",
    "        \"\"\"Extract tar and load sparse matrix with correct orientation\"\"\"\n",
    "        # Extract to subdirectory to avoid conflicts\n",
    "        extract_subdir = os.path.join(extract_dir, matrix_name)\n",
    "        Path(extract_subdir).mkdir(exist_ok=True)\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "            tar.extractall(extract_subdir)\n",
    "        print(f\"Loading {matrix_name} from {extract_subdir}\")\n",
    "        # Find matrix files recursively\n",
    "        matrix_file, features_file, barcodes_file = find_matrix_files(extract_subdir)\n",
    "        if not all([matrix_file, features_file, barcodes_file]):\n",
    "            print(f\"Missing files in {extract_subdir}:\")\n",
    "            print(f\"  matrix.mtx: {matrix_file}\")\n",
    "            print(f\"  features/genes.tsv: {features_file}\")\n",
    "            print(f\"  barcodes.tsv: {barcodes_file}\")\n",
    "            raise FileNotFoundError(\"Required 10X files not found\")\n",
    "        # Load sparse matrix - 10X format is features x barcodes (genes x cells)\n",
    "        matrix = mmread(matrix_file).tocsr()  # NO transpose - already correct orientation\n",
    "        # Load gene names\n",
    "        genes_df = pd.read_csv(features_file, sep='\\t', header=None)\n",
    "        if genes_df.shape[1] >= 2:\n",
    "            gene_names = genes_df.iloc[:, 1].values  # Gene symbols (column 2)\n",
    "        else:\n",
    "            gene_names = genes_df.iloc[:, 0].values  # Gene IDs (column 1)\n",
    "        # Load barcodes\n",
    "        barcodes = pd.read_csv(barcodes_file, sep='\\t', header=None).iloc[:, 0].values\n",
    "        print(f\"  Shape: {matrix.shape} (genes x cells)\")\n",
    "        print(f\"  Genes: {len(gene_names)}\")\n",
    "        print(f\"  Barcodes: {len(barcodes)}\")\n",
    "        # Validation - matrix should be genes x barcodes\n",
    "        if matrix.shape[0] != len(gene_names):\n",
    "            print(f\"WARNING: Matrix rows ({matrix.shape[0]}) != gene names ({len(gene_names)})\")\n",
    "        if matrix.shape[1] != len(barcodes):\n",
    "            print(f\"WARNING: Matrix cols ({matrix.shape[1]}) != barcodes ({len(barcodes)})\")\n",
    "        return matrix, gene_names, barcodes\n",
    "\n",
    "    # Load raw and filtered data\n",
    "    print(\"Loading raw matrix...\")\n",
    "    raw_counts, raw_gene_names, raw_barcodes = extract_and_load_matrix(raw_tar_path, \"raw\")\n",
    "    print(\"Loading filtered matrix...\")\n",
    "    filtered_counts, filt_gene_names, filt_barcodes = extract_and_load_matrix(filtered_tar_path, \"filtered\")\n",
    "\n",
    "    # Verify gene names match\n",
    "    if not np.array_equal(raw_gene_names, filt_gene_names):\n",
    "        print(\"Warning: Gene names don't match between raw and filtered data\")\n",
    "        print(f\"Raw genes: {len(raw_gene_names)}\")\n",
    "        print(f\"Filtered genes: {len(filt_gene_names)}\")\n",
    "        # Try to find common genes\n",
    "        common_genes = np.intersect1d(raw_gene_names, filt_gene_names)\n",
    "        print(f\"Common genes: {len(common_genes)}\")\n",
    "        if len(common_genes) > 0:\n",
    "            # Subset to common genes\n",
    "            raw_gene_idx = np.isin(raw_gene_names, common_genes)\n",
    "            filt_gene_idx = np.isin(filt_gene_names, common_genes)\n",
    "            raw_counts = raw_counts[raw_gene_idx, :]\n",
    "            filtered_counts = filtered_counts[filt_gene_idx, :]\n",
    "            raw_gene_names = raw_gene_names[raw_gene_idx]\n",
    "            filt_gene_names = filt_gene_names[filt_gene_idx]\n",
    "            print(f\"Subsetted to {len(common_genes)} common genes\")\n",
    "\n",
    "    # Clean up\n",
    "    import shutil\n",
    "    shutil.rmtree(extract_dir)\n",
    "    print(f\"\\nData loaded successfully:\")\n",
    "    print(f\"Raw data: {raw_counts.shape} ({raw_counts.nnz:,} non-zero entries)\")\n",
    "    print(f\"Filtered data: {filtered_counts.shape} ({filtered_counts.nnz:,} non-zero entries)\")\n",
    "\n",
    "    return raw_counts, filtered_counts, raw_gene_names, filt_gene_names, raw_barcodes, filt_barcodes\n",
    "\n",
    "# Define the paths to your PBMC3k data files\n",
    "raw_tar_path = \"./pbmc3k_raw_gene_bc_matrices.tar.gz\"\n",
    "filtered_tar_path = \"./pbmc3k_filtered_gene_bc_matrices.tar.gz\"\n",
    "\n",
    "# Load the data\n",
    "raw_counts, filtered_counts, raw_genes, filt_genes, raw_barcodes, filt_barcodes = load_10x_data_fixed(raw_tar_path, filtered_tar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4ca42-9afd-4570-be75-37ae76e29205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import scanpy as sc\n",
    "\n",
    "# Create an AnnData object for further processing\n",
    "adata = sc.AnnData(filtered_counts.T)  # Transpose to get cells x genes\n",
    "adata.var['gene_names'] = filt_genes\n",
    "adata.obs['barcodes'] = filt_barcodes\n",
    "\n",
    "# Perform basic preprocessing\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "adata = adata[:, adata.var.highly_variable]\n",
    "\n",
    "# Perform PCA\n",
    "sc.tl.pca(adata, svd_solver='arpack')\n",
    "\n",
    "# Compute the neighborhood graph\n",
    "sc.pp.neighbors(adata)\n",
    "\n",
    "# Perform UMAP for visualization\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "# Perform clustering\n",
    "sc.tl.leiden(adata)\n",
    "\n",
    "# Extract clusters for comparison\n",
    "clusters = adata.obs['leiden'].astype('category').cat.codes.values\n",
    "\n",
    "# Prepare data dictionary for comparison\n",
    "data_for_comparison = {\n",
    "    'raw': raw_counts,\n",
    "    'filtered': filtered_counts,\n",
    "    'genes': filt_genes,\n",
    "    'clusters': clusters,\n",
    "    'contamination': 0.1  # Example contamination level\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd90550-627e-46a6-bf5e-8b670c525c09",
   "metadata": {},
   "source": [
    "#### actual comparison here:\n",
    "\n",
    "!! only operates on HVG rn as it took forever - I think we gotta optimze a bit here and there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549ea43-4f58-4f38-bba1-79cc1c2914db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c3ccf-9f97-42d2-9e5f-8570421b800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decontx_core_with_real_data(adata, filtered_counts, filt_genes):\n",
    "    \"\"\"\n",
    "    Test core decontX algorithm using real PBMC data and scanpy clusters.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    from rpy2.robjects.conversion import localconverter\n",
    "    from rpy2.robjects import numpy2ri, default_converter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.sparse import issparse\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING DECONTX CORE ALGORITHM WITH REAL PBMC DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Extract clusters from scanpy analysis\n",
    "    clusters = adata.obs['leiden'].astype(int).values + 1  # Make 1-indexed for R\n",
    "    \n",
    "    # 2. Prepare count matrix\n",
    "    if issparse(filtered_counts):\n",
    "        counts_matrix = filtered_counts.T.toarray()  # cells x genes\n",
    "    else:\n",
    "        counts_matrix = filtered_counts.T  # cells x genes\n",
    "    \n",
    "    # ========== FILTER DATA FOR BOTH PYTHON AND R ==========\n",
    "    # Remove genes with zero counts across all cells\n",
    "    gene_sums = counts_matrix.sum(axis=0)\n",
    "    valid_genes = gene_sums > 0\n",
    "    counts_matrix_filtered = counts_matrix[:, valid_genes]\n",
    "    \n",
    "    # Remove cells with zero counts (shouldn't happen but check)\n",
    "    cell_sums = counts_matrix_filtered.sum(axis=1)\n",
    "    valid_cells = cell_sums > 0\n",
    "    counts_matrix_filtered = counts_matrix_filtered[valid_cells, :]\n",
    "    clusters_filtered = clusters[valid_cells]\n",
    "    \n",
    "    print(f\"\\nFiltering:\")\n",
    "    print(f\"  Original: {counts_matrix.shape[0]} cells, {counts_matrix.shape[1]} genes\")\n",
    "    print(f\"  Filtered: {counts_matrix_filtered.shape[0]} cells, {counts_matrix_filtered.shape[1]} genes\")\n",
    "    print(f\"  Removed {counts_matrix.shape[1] - counts_matrix_filtered.shape[1]} zero-count genes\")\n",
    "    print(f\"  Removed {counts_matrix.shape[0] - counts_matrix_filtered.shape[0]} zero-count cells\")\n",
    "    \n",
    "    # Use filtered data for BOTH implementations\n",
    "    counts_matrix = counts_matrix_filtered\n",
    "    clusters = clusters_filtered\n",
    "    \n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    n_cells = len(clusters)\n",
    "    n_genes = counts_matrix.shape[1]\n",
    "    \n",
    "    print(f\"\\nData summary after filtering:\")\n",
    "    print(f\"  Cells: {n_cells}\")\n",
    "    print(f\"  Genes: {n_genes}\")\n",
    "    print(f\"  Clusters: {n_clusters}\")\n",
    "    print(f\"  Cluster sizes: {np.bincount(clusters)[1:]}\")\n",
    "    \n",
    "    # Basic stats\n",
    "    total_umis = counts_matrix.sum(axis=1)\n",
    "    print(f\"  UMIs per cell: mean={total_umis.mean():.0f}, median={np.median(total_umis):.0f}\")\n",
    "    \n",
    "    # 3. Run Python DecontX with PROVIDED clusters\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"RUNNING PYTHON DECONTX\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    from decontx.model import DecontXModel\n",
    "    \n",
    "    # Test with different delta priors\n",
    "    delta_configs = [\n",
    "        ([10, 10], True, \"Default: delta=[10,10], estimate=True\"),\n",
    "        ([10, 10], False, \"Fixed: delta=[10,10], estimate=False\"),\n",
    "        ([90, 10], False, \"Low contamination prior: delta=[90,10]\"),\n",
    "    ]\n",
    "    \n",
    "    py_results = {}\n",
    "    \n",
    "    for delta, estimate, desc in delta_configs:\n",
    "        print(f\"\\n{desc}\")\n",
    "        model = DecontXModel(\n",
    "            max_iter=50,\n",
    "            delta=delta,\n",
    "            estimate_delta=estimate,\n",
    "            convergence=0.001,\n",
    "            seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        result = model.fit_transform(counts_matrix, clusters)\n",
    "        contamination = result['contamination']\n",
    "        \n",
    "        print(f\"  Mean contamination: {contamination.mean():.1%}\")\n",
    "        print(f\"  Std contamination:  {contamination.std():.1%}\")\n",
    "        print(f\"  Range: {contamination.min():.1%} - {contamination.max():.1%}\")\n",
    "        \n",
    "        py_results[desc] = contamination\n",
    "    \n",
    "    # 4. Run R DecontX with same clusters\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"RUNNING R DECONTX\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    ro.r('library(decontX)')\n",
    "    \n",
    "    r_results = {}\n",
    "    \n",
    "    with localconverter(default_converter + numpy2ri.converter):\n",
    "        # Convert data to R\n",
    "        # ro.globalenv['counts_r'] = ro.r.matrix(\n",
    "        #     ro.FloatVector(counts_matrix.T.flatten()),\n",
    "        #     nrow=n_genes,\n",
    "        #     ncol=n_cells\n",
    "        # )\n",
    "        # In your original comparison, transpose the matrix for R:\n",
    "        ro.globalenv['counts_r'] = ro.r.matrix(\n",
    "            ro.FloatVector(counts_matrix.flatten()),  # Don't transpose here\n",
    "            nrow=n_genes,  # genes as rows\n",
    "            ncol=n_cells   # cells as columns  \n",
    "        )\n",
    "        ro.globalenv['z_r'] = ro.IntVector(clusters)\n",
    "        \n",
    "        for delta, estimate, desc in delta_configs:\n",
    "            print(f\"\\n{desc}\")\n",
    "            \n",
    "            delta_r = f\"c({delta[0]}, {delta[1]})\"\n",
    "            estimate_r = \"TRUE\" if estimate else \"FALSE\"\n",
    "            \n",
    "            ro.r(f'''\n",
    "            set.seed(42)\n",
    "            result_r <- decontX(\n",
    "                x = counts_r, \n",
    "                z = z_r,\n",
    "                maxIter = 50,\n",
    "                delta = {delta_r},\n",
    "                estimateDelta = {estimate_r},\n",
    "                convergence = 0.001,\n",
    "                verbose = FALSE\n",
    "            )\n",
    "            contamination_r <- result_r$contamination\n",
    "            ''')\n",
    "            \n",
    "            contamination = np.array(ro.r('contamination_r'))\n",
    "            \n",
    "            print(f\"  Mean contamination: {contamination.mean():.1%}\")\n",
    "            print(f\"  Std contamination:  {contamination.std():.1%}\")\n",
    "            print(f\"  Range: {contamination.min():.1%} - {contamination.max():.1%}\")\n",
    "            \n",
    "            r_results[desc] = contamination\n",
    "    \n",
    "    # 5. Detailed comparison\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"DETAILED COMPARISON\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for desc in delta_configs:\n",
    "        desc_str = desc[2]\n",
    "        py_cont = py_results[desc_str]\n",
    "        r_cont = r_results[desc_str]\n",
    "        \n",
    "        correlation = np.corrcoef(py_cont, r_cont)[0, 1]\n",
    "        mae = np.mean(np.abs(py_cont - r_cont))\n",
    "        \n",
    "        print(f\"\\n{desc_str}:\")\n",
    "        print(f\"  Python mean: {py_cont.mean():.1%}\")\n",
    "        print(f\"  R mean:      {r_cont.mean():.1%}\")\n",
    "        print(f\"  Correlation: {correlation:.3f}\")\n",
    "        print(f\"  Mean Absolute Error: {mae:.1%}\")\n",
    "    \n",
    "    # 6. Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    for idx, desc in enumerate(delta_configs):\n",
    "        desc_str = desc[2]\n",
    "        py_cont = py_results[desc_str]\n",
    "        r_cont = r_results[desc_str]\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax = axes[0, idx]\n",
    "        ax.scatter(r_cont, py_cont, alpha=0.3, s=5)\n",
    "        ax.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "        ax.set_xlabel('R contamination')\n",
    "        ax.set_ylabel('Python contamination')\n",
    "        ax.set_title(f'{desc_str.split(\":\")[0]}')\n",
    "        ax.set_xlim([0, max(r_cont.max(), py_cont.max()) * 1.1])\n",
    "        ax.set_ylim([0, max(r_cont.max(), py_cont.max()) * 1.1])\n",
    "        \n",
    "        # Histogram\n",
    "        ax = axes[1, idx]\n",
    "        ax.hist(r_cont, bins=30, alpha=0.5, label='R', density=True)\n",
    "        ax.hist(py_cont, bins=30, alpha=0.5, label='Python', density=True)\n",
    "        ax.set_xlabel('Contamination')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('decontx_comparison_real_data.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Per-cluster analysis\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PER-CLUSTER CONTAMINATION (Default settings)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    py_default = py_results[delta_configs[0][2]]\n",
    "    r_default = r_results[delta_configs[0][2]]\n",
    "    \n",
    "    print(\"\\nCluster | Python Mean | R Mean | Difference\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for cluster_id in np.unique(clusters):\n",
    "        mask = clusters == cluster_id\n",
    "        py_mean = py_default[mask].mean()\n",
    "        r_mean = r_default[mask].mean()\n",
    "        diff = py_mean - r_mean\n",
    "        print(f\"   {cluster_id:2d}   |   {py_mean:6.1%}    | {r_mean:6.1%} |  {diff:+6.1%}\")\n",
    "    \n",
    "    # 8. Check cells with extreme differences\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"CELLS WITH LARGEST DIFFERENCES\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    diffs = np.abs(py_default - r_default)\n",
    "    top_diff_idx = np.argsort(diffs)[-10:][::-1]\n",
    "    \n",
    "    print(\"\\nCell | Cluster | Python | R      | Diff   | UMIs\")\n",
    "    print(\"-\" * 55)\n",
    "    for idx in top_diff_idx[:5]:\n",
    "        print(f\"{idx:4d} | {clusters[idx]:7d} | {py_default[idx]:6.1%} | {r_default[idx]:6.1%} | \"\n",
    "              f\"{diffs[idx]:6.1%} | {total_umis[idx]:4.0f}\")\n",
    "    \n",
    "    return py_results, r_results, clusters\n",
    "\n",
    "# Run the test with your data\n",
    "py_results, r_results, clusters = test_decontx_core_with_real_data(\n",
    "    adata, \n",
    "    filtered_counts, \n",
    "    filt_genes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9f0da-d95c-4538-b904-acd11ba23c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decontx_timing_and_diagnostics(adata, filtered_counts, filt_genes):\n",
    "    \"\"\"\n",
    "    Minimal diagnostics for timing and difference identification.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import rpy2.robjects as ro\n",
    "    from rpy2.robjects.conversion import localconverter\n",
    "    from rpy2.robjects import numpy2ri, default_converter\n",
    "    from scipy.sparse import issparse\n",
    "    \n",
    "    # Prepare data (same as before)\n",
    "    clusters = adata.obs['leiden'].astype(int).values + 1\n",
    "    if issparse(filtered_counts):\n",
    "        counts_matrix = filtered_counts.T.toarray()\n",
    "    else:\n",
    "        counts_matrix = filtered_counts.T\n",
    "    \n",
    "    # Filter genes\n",
    "    gene_sums = counts_matrix.sum(axis=0)\n",
    "    valid_genes = gene_sums > 0\n",
    "    counts_matrix = counts_matrix[:, valid_genes]\n",
    "    \n",
    "    print(f\"Testing with {counts_matrix.shape[0]} cells, {counts_matrix.shape[1]} genes\")\n",
    "    \n",
    "    # 1. TIMING COMPARISON\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"TIMING COMPARISON (50 iterations)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Python timing\n",
    "    from decontx.model import DecontXModel\n",
    "    \n",
    "    start = time.time()\n",
    "    model = DecontXModel(\n",
    "        max_iter=50,\n",
    "        delta=[10, 10],\n",
    "        estimate_delta=True,\n",
    "        convergence=0.001,\n",
    "        seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    py_result = model.fit_transform(counts_matrix, clusters)\n",
    "    py_time = time.time() - start\n",
    "    py_contamination = py_result['contamination']\n",
    "    \n",
    "    print(f\"Python DecontX: {py_time:.2f} seconds\")\n",
    "    \n",
    "    # R timing (excluding rpy2 overhead)\n",
    "    ro.r('library(decontX)')\n",
    "    \n",
    "    with localconverter(default_converter + numpy2ri.converter):\n",
    "        ro.globalenv['counts_r'] = ro.r.matrix(\n",
    "            ro.FloatVector(counts_matrix.flatten()),\n",
    "            nrow=counts_matrix.shape[1],\n",
    "            ncol=counts_matrix.shape[0]\n",
    "        )\n",
    "        ro.globalenv['z_r'] = ro.IntVector(clusters)\n",
    "        \n",
    "        # Time only the R execution\n",
    "        start = time.time()\n",
    "        ro.r('''\n",
    "        set.seed(42)\n",
    "        result_r <- decontX(\n",
    "            x = counts_r, \n",
    "            z = z_r,\n",
    "            maxIter = 50,\n",
    "            delta = c(10, 10),\n",
    "            estimateDelta = TRUE,\n",
    "            convergence = 0.001,\n",
    "            verbose = FALSE\n",
    "        )\n",
    "        ''')\n",
    "        r_time = time.time() - start\n",
    "        \n",
    "        r_contamination = np.array(ro.r('result_r$contamination'))\n",
    "    \n",
    "    print(f\"R DecontX: {r_time:.2f} seconds\")\n",
    "    print(f\"Speed ratio (Python/R): {py_time/r_time:.2f}x\")\n",
    "    \n",
    "    # 2. DIAGNOSTIC FOR DIFFERENCES\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"DIFFERENCE DIAGNOSTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    diffs = np.abs(py_contamination - r_contamination)\n",
    "    \n",
    "    # Which cells have largest differences?\n",
    "    large_diff_mask = diffs > 0.05  # >5% difference\n",
    "    n_large_diff = np.sum(large_diff_mask)\n",
    "    \n",
    "    print(f\"\\nCells with >5% difference: {n_large_diff}/{len(diffs)} ({100*n_large_diff/len(diffs):.1f}%)\")\n",
    "    \n",
    "    if n_large_diff > 0:\n",
    "        # Analyze characteristics of high-difference cells\n",
    "        total_umis = counts_matrix.sum(axis=1)\n",
    "        \n",
    "        print(\"\\nCharacteristics of high-difference cells:\")\n",
    "        print(f\"  Mean UMIs (high diff): {total_umis[large_diff_mask].mean():.0f}\")\n",
    "        print(f\"  Mean UMIs (low diff):  {total_umis[~large_diff_mask].mean():.0f}\")\n",
    "        \n",
    "        # Check if specific clusters are problematic\n",
    "        for cluster_id in np.unique(clusters):\n",
    "            cluster_mask = clusters == cluster_id\n",
    "            cluster_diff = diffs[cluster_mask].mean()\n",
    "            if cluster_diff > 0.02:\n",
    "                print(f\"  Cluster {cluster_id}: mean diff = {cluster_diff:.3f}\")\n",
    "    \n",
    "    # 3. PARAMETER CONVERGENCE CHECK\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Check if delta estimation converged similarly\n",
    "    py_delta = py_result['delta']\n",
    "    r_delta = np.array(ro.r('result_r$runParams$delta'))\n",
    "    \n",
    "    print(f\"Final delta (Python): [{py_delta[0]:.2f}, {py_delta[1]:.2f}]\")\n",
    "    print(f\"Final delta (R):      [{r_delta[0]:.2f}, {r_delta[1]:.2f}]\")\n",
    "    \n",
    "    # Check log-likelihood trajectories if available\n",
    "    if 'log_likelihood' in py_result:\n",
    "        py_ll = py_result['log_likelihood']\n",
    "        print(f\"Final log-likelihood (Python): {py_ll[-1]:.0f}\")\n",
    "        \n",
    "    return py_contamination, r_contamination, py_time, r_time\n",
    "\n",
    "py_contamination, r_contamination, py_time, r_time = test_decontx_timing_and_diagnostics(adata, filtered_counts, filt_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c61d28-e2a9-4789-a101-08bc269e664a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6965-b0c7-47ce-a546-e346525c04e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5576e-1a38-4b30-aaa0-2744d5de7fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd7139-3de0-400d-8697-0efcff274991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f790c4-b3b0-43b5-95af-80fadb01cf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48423ead-7c05-4e2f-b14c-9eb1c853dff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a0cb7-b141-44e6-a5d1-fc70287f994d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
